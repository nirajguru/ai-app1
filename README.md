# Invoking LLMs locally

In this repo, I will use ollama to install LLMs locally on my machine and chat with LLMs using APIs.
 